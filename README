NODE: I am sorting out a few issues with transfer of updated code on github. I was ill this week, so it will take a few more days. AP (29 Nov 2020)


REQUIREMENTS:

- A complete gnu C++ compiler
- LibTiff
- libblas, liblapack
- The MPI library installed
- A python interpreter
- For data analysis, the R interpreter


INSTALLING

The installer will download required data from the WorldPop database.
Data for one country could range within 5-10Gb.
Downloading might take a long time depending on your connection.
You can repeat the install operation for different countries or
for different number of cores or grid resolutions.

- Edit the install.sh file and modify the following parameters:
  CORES: the number of cores used by each simulation.
  GRIDRES: the resolution in km.
  COUNTRY: the name of the country of interest.
  OUTDIR: name of the directory where to store results:
          this will be a sub directory of the Output directory.

- Run the installer with
  bash install.sh



SET UP

Once installed, move to the 'Master' directory:

cd Master

Edit the 'Makefile' to alter the following variables:
CORES : the number of cores each run will use.
COUNTRY: the country you want to work with
OUTDIR: the directory where executable and data should be stored.
        This will be a sub-directory of 'Output'.

Edit the 'config.py' file and alter the parameters of the country you intend
        to work with.  The 'Default' country can be used as a blueprint.
		You only need to specify values that differ from the 'Default' values.

Now use the make command as follows:

make prepare  # This will remove any pre-existing data in the target output directory.
              # Use with caution.

make  # This will generate three executables that can be used to run specific conditions
      # or to fit to specific data



EXECUTING

Move to the chosen output directory

cd ../Output/XYZ

where XYZ represents the name of the output directory chosen during the SET UP phase.

- Multirun
  Edit the 'scriptruns.sh' bash script with the correct execution commands for your cluster.
  Submit

- Fitting
  Edit the 'scriptfits.sh' bash script with the correct execution commands for your cluster.
  Specify the number of replicas (see below).
  Submit



NOTES about fitting:

You can now use multiple nodes for parallel fitting by using replicas.  The number of replicas
should be an integer divisor of the number of processes.  For instance, suppose that you want
to use 10 cluster nodes, each having 16 cores.  That gives a total of 160 processes.
You can use 5 replicas, 10 replicas, but not 7 replicas.  For efficiency, you might want to 
have one replica per node, thus running 10 parallel replicas each using 16 cores.
You will only need to alter the 'scriptfits.sh' file with

  REPLICAS=10

and, according to the node manager of your cluster, either

  #PBS  -l nodes=10:ppm=16

or 

  #SBATCH --nodes=10
  #SBATCH --ntasks-per-node=16




